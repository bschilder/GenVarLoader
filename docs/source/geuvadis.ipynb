{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import genvarloader as gvl\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seqpro as sp\n",
    "import pooch\n",
    "from loguru import logger\n",
    "from einops import rearrange\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Geuvadis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll see how to use GenVarLoader (GVL) to:\n",
    "\n",
    "1. Write a GVL dataset\n",
    "2. Add transforms\n",
    "3. Lazily subset it (train/test splits)\n",
    "4. Get a PyTorch DataLoader\n",
    "5. Cache transformed tracks on disk (optional)\n",
    "6. Evaluate Basenji2 across genes and individuals (optional)\n",
    "\n",
    "This tutorial also assumes you have read [\"What's a gvl.Dataset?\"](https://genvarloader.readthedocs.io/en/stable/dataset.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note on logging: GenVarLoader uses [loguru](https://loguru.readthedocs.io/en/stable/index.html) for logging. We will enable it at the \"INFO\" level to get some additional information from GVL for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "logger.enable(\"genvarloader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Geuvadis dataset is 451 individuals from the 1000 Genomes Project that have both whole genome sequencing and RNA-seq from blood samples. We'll see how to use GVL to get a high performance dataloader that yields haplotypes and tracks for training or running inference with sequence models. For the sake of this tutorial, we'll only work with chromosome 22 so everything can run in a few minutes.\n",
    "\n",
    "Downloading this data should take ~5-10 minutes and is the slowest step in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRCh38 chromosome 22 sequence\n",
    "reference = pooch.retrieve(\n",
    "    url=\"https://ftp.ensembl.org/pub/release-112/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\",\n",
    "    known_hash=\"sha256:974f97ac8ef7ffae971b63b47608feda327403be40c27e391ee4a1a78b800df5\",\n",
    "    progressbar=True,\n",
    ")\n",
    "if not Path(f\"{reference[:-3]}.bgz\").exists():\n",
    "    !gzip -dc {reference} | bgzip > {reference[:-3]}.bgz\n",
    "reference = reference[:-3] + \".bgz\"\n",
    "\n",
    "# PLINK 2 files\n",
    "variants = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.pgen\",\n",
    "    known_hash=\"md5:31aba970e35f816701b2b99118dfc2aa\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.pgen\",\n",
    ")\n",
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.psam\",\n",
    "    known_hash=\"md5:eefa7aad5acffe62bf41df0a4600129c\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.psam\",\n",
    ")\n",
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/1kGP.chr22.pvar\",\n",
    "    known_hash=\"md5:5f922af91c1a2f6822e2f1bb4469d12b\",\n",
    "    progressbar=True,\n",
    "    fname=\"1kGP.chr22.pvar\",\n",
    ")\n",
    "\n",
    "# BigWigs and sample ID mapping\n",
    "bw_paths = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/bw_chr22.tar.gz\",\n",
    "    known_hash=\"md5:14bf72e9e9d3e2318d07315c4a2675fb\",\n",
    "    progressbar=True,\n",
    "    processor=pooch.Untar(),\n",
    ")\n",
    "bw_table_path = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/bigwig_table.csv\",\n",
    "    known_hash=\"md5:7fe7c55b61c7dfa66cfd0a49336f3b08\",\n",
    "    progressbar=True,\n",
    ")\n",
    "\n",
    "# BED\n",
    "bed_path = pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.13656224/chr22_egenes.bed\",\n",
    "    known_hash=\"md5:ccb55548e4ddd416d50dbe6638459421\",\n",
    "    progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the GVL dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll specify a directory to store the dataset (similar to Zarr stores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = TemporaryDirectory(suffix=\".gvl\")\n",
    "ds_path = tmp_dir.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a table or dictionary specifying the sample names for each BigWig. Tables must have at least have columns `sample` and `path` as seen below. The join is added here to update the paths to match the actual download paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sample</th><th>read_count</th><th>path</th></tr><tr><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;HG00236&quot;</td><td>34548283</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;HG00259&quot;</td><td>53041143</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20519&quot;</td><td>36620358</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20811&quot;</td><td>24398971</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr><tr><td>&quot;NA20768&quot;</td><td>30019566</td><td>&quot;/carter/users/dlaub/.cache/poo…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌─────────┬────────────┬─────────────────────────────────┐\n",
       "│ sample  ┆ read_count ┆ path                            │\n",
       "│ ---     ┆ ---        ┆ ---                             │\n",
       "│ str     ┆ i64        ┆ str                             │\n",
       "╞═════════╪════════════╪═════════════════════════════════╡\n",
       "│ HG00236 ┆ 34548283   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ HG00259 ┆ 53041143   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20519 ┆ 36620358   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20811 ┆ 24398971   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "│ NA20768 ┆ 30019566   ┆ /carter/users/dlaub/.cache/poo… │\n",
       "└─────────┴────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigwig_table = (\n",
    "    pl.read_csv(bw_table_path)\n",
    "    .join(\n",
    "        pl.Series(bw_paths).to_frame(\"realpath\"),\n",
    "        left_on=\"path\",\n",
    "        right_on=pl.col(\"realpath\").str.split(\"/\").list.get(-1),\n",
    "    )\n",
    "    .drop(\"path\")\n",
    "    .rename({\"realpath\": \"path\"})\n",
    ")\n",
    "bigwig_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll need a BED file specifying what regions to include in the dataset. We can either specify a path or a polars DataFrame. We'll use [gvl.read_bedlike](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.read_bedlike) to conveniently read the BED file into memory and subset it to just the top 20 eGenes for this tutorial. The BED file provided corresponds to transcription start sites of eGenes, sorted in descending order by their absolute sum of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>chrom</th><th>chromStart</th><th>chromEnd</th><th>name</th><th>score</th><th>strand</th></tr><tr><td>str</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;chr22&quot;</td><td>41699499</td><td>41699499</td><td>&quot;ENSG00000167077&quot;</td><td>null</td><td>&quot;+&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>42835412</td><td>42835412</td><td>&quot;ENSG00000100266&quot;</td><td>null</td><td>&quot;-&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>20858983</td><td>20858983</td><td>&quot;ENSG00000099940&quot;</td><td>null</td><td>&quot;+&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>20707691</td><td>20707691</td><td>&quot;ENSG00000241973&quot;</td><td>null</td><td>&quot;-&quot;</td></tr><tr><td>&quot;chr22&quot;</td><td>49918167</td><td>49918167</td><td>&quot;ENSG00000184164&quot;</td><td>null</td><td>&quot;+&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 6)\n",
       "┌───────┬────────────┬──────────┬─────────────────┬───────┬────────┐\n",
       "│ chrom ┆ chromStart ┆ chromEnd ┆ name            ┆ score ┆ strand │\n",
       "│ ---   ┆ ---        ┆ ---      ┆ ---             ┆ ---   ┆ ---    │\n",
       "│ str   ┆ i64        ┆ i64      ┆ str             ┆ f64   ┆ str    │\n",
       "╞═══════╪════════════╪══════════╪═════════════════╪═══════╪════════╡\n",
       "│ chr22 ┆ 41699499   ┆ 41699499 ┆ ENSG00000167077 ┆ null  ┆ +      │\n",
       "│ chr22 ┆ 42835412   ┆ 42835412 ┆ ENSG00000100266 ┆ null  ┆ -      │\n",
       "│ chr22 ┆ 20858983   ┆ 20858983 ┆ ENSG00000099940 ┆ null  ┆ +      │\n",
       "│ chr22 ┆ 20707691   ┆ 20707691 ┆ ENSG00000241973 ┆ null  ┆ -      │\n",
       "│ chr22 ┆ 49918167   ┆ 49918167 ┆ ENSG00000184164 ┆ null  ┆ +      │\n",
       "└───────┴────────────┴──────────┴─────────────────┴───────┴────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bed = gvl.read_bedlike(bed_path)[:20]\n",
    "bed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write the dataset.\n",
    "\n",
    "The `bed` above specifies the transcription start site for each gene so chromStart == chromEnd, so we'll expand those regions to $2^{17}$ (131,072) bp using [gvl.with_length](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.with_length) which corresponds to the input length for Basenji2.\n",
    "\n",
    "We'll also instantiate a [gvl.BigWigs](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.BigWigs) from the above table (we could also use a dictionary). We'll name this track \"read-depth\" so we can manage different transformations of the track data or provide multiple tracks for the same samples. Later, we'll add a transformed track for $\\log_2(\\text{CPM}+1)$ to see this in action.\n",
    "\n",
    "Finally, we'll pass `max_jitter` as 128 bp. This will allow random jittering of the sequences and tracks up to 128 bp in either direction. When we open the dataset later it will use the maximum amount of jitter by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 12:43:42.506\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mWriting dataset to /tmp/tmp4wvh6j90.gvl\u001b[0m\n",
      "\u001b[32m2025-03-19 12:43:42.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m104\u001b[0m - \u001b[1mFound existing GVL store, overwriting.\u001b[0m\n",
      "\u001b[32m2025-03-19 12:43:42.677\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m172\u001b[0m - \u001b[1mUsing 451 samples.\u001b[0m\n",
      "\u001b[32m2025-03-19 12:43:42.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mWriting genotypes.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf022e9d6434d53b8c3bc2e45ea8774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 12:44:08.634\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mWriting BigWig intervals.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4566f0918dd41ce8e4c15e68a0e1866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca09e6001927475cb1b90201e32511ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 12:44:12.885\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._write\u001b[0m:\u001b[36mwrite\u001b[0m:\u001b[36m204\u001b[0m - \u001b[1mFinished writing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "gvl.write(\n",
    "    path=ds_path,\n",
    "    bed=gvl.with_length(bed, 2**17),  # change region length to 131,072 bp\n",
    "    variants=variants,\n",
    "    bigwigs=gvl.BigWigs.from_table(name=\"read-depth\", table=bigwig_table),\n",
    "    max_jitter=128,  # allow up to 128 bp jitter\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that [gvl.write](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.write) will also automatically use the intersection of samples from source files. In this case, they are perfectly matched to each other. But, if we had used PLINK files for the full 3,202 samples from the 1000 Genomes Project then it would have identified and used the 451 intersecting samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is written, we can add a transform, split it, and get a PyTorch dataloader in ~10 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 12:44:12.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._impl\u001b[0m:\u001b[36mopen\u001b[0m:\u001b[36m387\u001b[0m - \u001b[1mLoading reference genome into memory. This typically has a modest memory footprint (a few GB) and greatly improves performance.\u001b[0m\n",
      "\u001b[32m2025-03-19 12:44:12.987\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset._impl\u001b[0m:\u001b[36mopen\u001b[0m:\u001b[36m455\u001b[0m - \u001b[1mOpened dataset:\n",
      "GVL store at /tmp/tmp4wvh6j90.gvl\n",
      "Is subset: False\n",
      "# of regions: 20\n",
      "# of samples: 451\n",
      "Jitter: 0 (max: 128)\n",
      "Sequence type: reference [haplotypes] annotated\n",
      "Active tracks: read-depth\n",
      "Tracks available: read-depth\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def transform(haps, tracks):\n",
    "    haps = rearrange(\n",
    "        sp.DNA.ohe(haps), \"... length alphabet -> ... alphabet length\"\n",
    "    ).astype(np.float32)\n",
    "    return haps, tracks\n",
    "\n",
    "\n",
    "ds = (\n",
    "    gvl.Dataset.open(ds_path, reference=reference)\n",
    "    .with_seqs(\"haplotypes\")\n",
    "    .with_tracks(\"read-depth\")\n",
    "    .with_len(2**17)\n",
    "    .with_transform(transform)\n",
    ")\n",
    "n_train = round(ds.n_samples * 0.8)\n",
    "gene1_train_ds = ds.subset_to(samples=slice(0, n_train))\n",
    "dl = gene1_train_ds.to_dataloader(batch_size=16, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GVL uses numba JIT compiled functions extensively, so the first call to `gvl.write`, first batch from a dataloader, etc. will often take much longer than subsequent calls due to compilation. This allows GVL to be multithreaded almost everywhere that it can be, so using `num_workers=0` or `1` is usually the best choice for dataloader throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 4, 131072]) torch.Size([16, 1, 2, 131072])\n"
     ]
    }
   ],
   "source": [
    "haps, tracks = next(iter(dl))\n",
    "print(haps.shape, tracks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one-hot encoding, the haplotypes have shape `(batch, ploidy, alphabet, length)` and the tracks have shape `(batch, tracks, ploidy, length)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-computing transformed tracks (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we would like to normalize the read depth across the dataset to account for library size. We could compute this on-the-fly, but GVL also offers a way to write this data back to disk to cache this computation and potentially improve performance. Note that this is the most technical part of this tutorial, so feel free to skip this and come back later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27256165, 43941108, 39687917, 22341838, 23258231])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_library_sizes = (\n",
    "    pl.Series(ds.samples)\n",
    "    .to_frame(\"sample\")\n",
    "    .join(bigwig_table, on=\"sample\", how=\"left\")[\"read_count\"]\n",
    "    .to_numpy()\n",
    ")\n",
    "sample_library_sizes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step, we'll use [Dataset.write_transformed_track](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Dataset.write_transformed_track) which expects a transform function to be given. From the docs:\n",
    "\n",
    "> The arguments given to the transform will be the dataset indices, region indices, and sample indices as numpy arrays and the tracks themselves as a [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array with shape `(regions, samples)`. The tracks must be a [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array since regions may be different lengths to accomodate indels. This function should then return the transformed tracks as a [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) array with the same shape and lengths.\n",
    "\n",
    "Below, you can see an example of a transform of ragged data that uses Numba to accelerate the computation. Note that working with [Ragged](https://genvarloader.readthedocs.io/en/latest/api.html#genvarloader.Ragged) arrays is often not necessary with on-the-fly transformations, since data for deep learning is readily processed to be uniform length before any transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f1efec91524ea79ed7be1b16d352b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GVL store at /tmp/tmp4wvh6j90.gvl\n",
       "Is subset: False\n",
       "# of regions: 20\n",
       "# of samples: 451\n",
       "Jitter: 0 (max: 128)\n",
       "Sequence type: reference [haplotypes] annotated\n",
       "Active tracks: read-depth\n",
       "Tracks available: lcpb, read-depth"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nb.njit(parallel=True, nogil=True, fastmath=True)\n",
    "def inner_transform(s_idx, data, offsets):\n",
    "    log_cpm = np.empty_like(data)\n",
    "    for i in nb.prange(len(offsets) - 1):\n",
    "        start = offsets[i]\n",
    "        end = offsets[i + 1]\n",
    "        sample = s_idx[i]\n",
    "        log_cpm[start:end] = np.log1p(\n",
    "            data[start:end] / sample_library_sizes[sample] * 1e6\n",
    "        )\n",
    "    return log_cpm\n",
    "\n",
    "\n",
    "def log_cpm(r_idx, s_idx, tracks: gvl.Ragged[np.float32]):\n",
    "    data = inner_transform(s_idx, tracks.data, tracks.offsets)\n",
    "    return gvl.Ragged.from_offsets(data, tracks.shape, tracks.offsets)\n",
    "\n",
    "ds = ds.write_transformed_track(\n",
    "    \"lcpb\", \"read-depth\", log_cpm, overwrite=True, max_mem=1 * 2**30\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the above cell crashes the kernel, it may have ran out of RAM which reducing `max_mem` can fix.**\n",
    "\n",
    "After writing the transformed track to disk, we can see the dataset now has the `\"lcpb\"` track available (note the list of available tracks is always sorted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Basenji2 on personalized expression (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this section requires PyTorch and basenji2-pytorch to be installed.\n",
    "\n",
    "Here, we'll show a (very) quick and dirty demo of some of the results found by [Huang et al. Nat Gen 2023](https://www.nature.com/articles/s41588-023-01574-w) with Basenji2. We also recommend running this with a GPU since inference with Basenji2 may take quite a while otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>genome</th><th>identifier</th><th>file</th><th>clip</th><th>scale</th><th>sum_stat</th><th>description</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>5110</td><td>0</td><td>&quot;CNhs12333&quot;</td><td>&quot;/home/drk/tillage/datasets/hum…</td><td>384</td><td>1</td><td>&quot;sum&quot;</td><td>&quot;CAGE:B lymphoblastoid cell lin…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 8)\n",
       "┌───────┬────────┬────────────┬─────────────────────────┬──────┬───────┬──────────┬────────────────┐\n",
       "│ index ┆ genome ┆ identifier ┆ file                    ┆ clip ┆ scale ┆ sum_stat ┆ description    │\n",
       "│ ---   ┆ ---    ┆ ---        ┆ ---                     ┆ ---  ┆ ---   ┆ ---      ┆ ---            │\n",
       "│ i64   ┆ i64    ┆ str        ┆ str                     ┆ i64  ┆ i64   ┆ str      ┆ str            │\n",
       "╞═══════╪════════╪════════════╪═════════════════════════╪══════╪═══════╪══════════╪════════════════╡\n",
       "│ 5110  ┆ 0      ┆ CNhs12333  ┆ /home/drk/tillage/datas ┆ 384  ┆ 1     ┆ sum      ┆ CAGE:B         │\n",
       "│       ┆        ┆            ┆ ets/hum…                ┆      ┆       ┆          ┆ lymphoblastoid │\n",
       "│       ┆        ┆            ┆                         ┆      ┆       ┆          ┆ cell lin…      │\n",
       "└───────┴────────┴────────────┴─────────────────────────┴──────┴───────┴──────────┴────────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_targets = pl.read_csv(\n",
    "    \"https://github.com/calico/basenji/blob/master/manuscripts/cross2020/targets_human.txt?raw=true\",\n",
    "    separator=\"\\t\",\n",
    ")\n",
    "target = human_targets.filter(\n",
    "    pl.col(\"description\").str.contains(r\"(?i)cage.*gm12878\")\n",
    ").item(0, \"index\")\n",
    "human_targets.filter(pl.col(\"description\").str.contains(r\"(?i)cage.*gm12878\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the above cell is taking more than a few seconds, try restarting its execution -- sometimes GitHub fails to respond so the file doesn't download. Likewise for below, recount3 can get stuck.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 451)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df = pl.read_csv(\n",
    "    \"https://duffel.rail.bio/recount3/human/data_sources/sra/gene_sums/42/ERP001942/sra.gene_sums.ERP001942.G029.gz\",\n",
    "    separator=\"\\t\",\n",
    "    comment_prefix=\"#\",\n",
    ")\n",
    "accessions = bigwig_table.with_columns(\n",
    "    accession=pl.col(\"path\").str.extract(r\"(ERR\\d+)\")\n",
    ")[\"accession\"]\n",
    "counts = (\n",
    "    bed.join(\n",
    "        count_df.select(\"gene_id\", *accessions),\n",
    "        left_on=\"name\",\n",
    "        right_on=pl.col(\"gene_id\").str.split(\".\").list.get(0),\n",
    "        maintain_order=\"left\",\n",
    "    )\n",
    "    .select(*accessions)\n",
    "    .to_numpy()\n",
    ")\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from basenji2_pytorch import Basenji2, basenji2_params, basenji2_weights\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "basenji2 = Basenji2(basenji2_params[\"model\"]).to(device)\n",
    "basenji2.load_state_dict(torch.load(basenji2_weights(), weights_only=True))\n",
    "basenji2.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-06 13:07:34.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mLoading reference genome into memory. This typically has a modest memory footprint (a few GB) and greatly improves performance.\u001b[0m\n",
      "\u001b[32m2025-03-06 13:07:34.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgenvarloader._dataset\u001b[0m:\u001b[36m_open\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1m\n",
      "GVL store tmpjtdkl6hx.gvl\n",
      "Is subset: False\n",
      "# of regions: 20\n",
      "# of samples: 451\n",
      "Max jitter: 128\n",
      "Genotypes available: Phased\n",
      "Tracks available: ['lcpb', 'read-depth']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def transform(haps, *args):\n",
    "    haps = rearrange(\n",
    "        sp.DNA.ohe(haps), \"... length alphabet -> ... alphabet length\"\n",
    "    ).astype(np.float32)\n",
    "    return haps, *args\n",
    "\n",
    "\n",
    "ds = gvl.Dataset.open(\n",
    "    ds_path,\n",
    "    reference=reference,\n",
    "    output_length=2**17,\n",
    "    transform=transform,\n",
    "    return_tracks=False,\n",
    "    return_indices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you're using a GPU, you may need to use a smaller batch size depending on how much GPU RAM you have.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "n_bins = (\n",
    "    896  # number of output bins for Basenji2, each corresponds to 128 bp of sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predictions for all genes using reference sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27abf77086bf40539ca1f7ecb18aff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_preds = np.empty((ds.n_regions, n_bins), dtype=np.float32)\n",
    "with torch.no_grad():\n",
    "    for ref, r_idx, _ in tqdm(\n",
    "        ds.subset_to(samples=0)\n",
    "        .with_settings(return_sequences=\"reference\")\n",
    "        .to_dataloader(batch_size=batch_size, num_workers=0)\n",
    "    ):\n",
    "        ref_preds[r_idx] = basenji2(ref.to(\"cuda\"))[..., target].numpy(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll compute the Pearson correlation between predicted transformed CAGE-seq read-depth and mean expression. We'll use a 5 bin (640 bp) window that is 9 bins (1152 bp) upstream of the TSS since this yielded the highest correlation with a little testing. This is somewhat expected since CAGE-seq reads should fall in the 5' UTR region, and we haven't thoroughly confirmed that the TSS coordinates we're using are exactly the same as what Basenji2 trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.47156382152055587)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_x_gene = np.corrcoef(\n",
    "    ref_preds[..., 896 // 2 - 9 : 896 // 2 - 4].mean(-1), counts.mean(-1), rowvar=False\n",
    ")[0, 1]\n",
    "ref_x_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd expect this to be the highest possible correlation Basenji2 can achieve on these genes. Let's see how it does across individuals and across genes with haplotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01aef573a534e73817f0b7ed3ca7ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = np.empty(ds.full_shape + (n_bins,), dtype=np.float64)\n",
    "with torch.no_grad():\n",
    "    for haps, r_idx, s_idx in tqdm(\n",
    "        ds.to_dataloader(batch_size=batch_size, num_workers=0)\n",
    "    ):\n",
    "        preds[r_idx, s_idx] = basenji2(haps[:, 0].to(device))[..., target].numpy(\n",
    "            force=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.005341827841689825), np.float64(0.4334261598507151))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ave_pearson_x_idv = np.diag(\n",
    "    np.corrcoef(preds[..., 896 // 2 - 9 : 896 // 2 - 4].mean(-1), counts), 20\n",
    ").mean()\n",
    "ave_pearson_x_gene = np.diag(\n",
    "    np.corrcoef(preds[..., 896 // 2 - 9 : 896 // 2 - 4].mean(-1), counts, rowvar=False),\n",
    "    451,\n",
    ").mean()\n",
    "ave_pearson_x_idv, ave_pearson_x_gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average correlation across genes with haplotypes is only slightly less than with reference sequences, but just as Huang et al. and others have found, the correlation across individuals is 0 on average."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GVL Docs",
   "language": "python",
   "name": "gvl-docs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
